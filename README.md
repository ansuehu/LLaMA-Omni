**LLama-Omni Training Code Reproduction**

1. Install the environment following LLama-Omni's instructions (https://github.com/ictnlp/LLaMA-Omni)

2. The `wavs` directory contains 100 data points generated by the paper's method (same prompt, using Qwen model, all other parameters consistent), for training in Phase 1 and Phase 2

3. Place the Whisper model in the `models` directory (requires separate download); For Phase 1, I used the 1B model (Llama-3.2-1B-Instruct). After downloading, place it in the current directory and remember to modify the whisper configuration in config.json, otherwise errors will occur. If you can run the 8B model, you can directly use the weights from the original paper or modify the native 8B model.

4. vocoder is the weight for the audio generation module (included).

5. Stage 2 data is located at omni_speech/infer/gen_answer_data/answer.json. It contains tokens generated from responses produced by the audio files in the `wavs` directory.

6. Both stages use BF16 precision for accuracy. Loss decreases normally. On a single 3090 GPU, Stage 1 can utilize multiple GPUs due to its 1B model size. Stage 2 was only tested on a single GPU due to hardware limitations but converged to 2. Using fp16 causes loss nan issues.

7. Launch methods:

    Stage 1: bash omni_speech/train/run.sh

    Stage 2: python omni_speech/train/stage2.py    

PS:

1. Thanks to the LLama-Omni team! Also grateful to my colleague @EDGSCOUT-li for collaborating on reproducing this paper's training process. We plan to replicate freeze_omni (https://github.com/VITA-MLLM/Freeze-Omni).

We'll promptly address any issues raised and update this project accordingly.

 2. Due to resource constraints, we haven't yet trained and released a functional model using larger datasets. Future work may involve training an end-to-end Chinese dialogue model using freeze methods.

3. We hope this provides some assistance to those working on end-to-end speech dialogue systems.



python ./dataset_generation/download_data.py

sbatch ./dataset_generation/instruction_rewriting.slurm

Translated with DeepL.com (free version)
